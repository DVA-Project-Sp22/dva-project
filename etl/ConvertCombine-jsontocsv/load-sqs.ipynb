{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Loading SQS with Messages for Lambda Processing\n",
    "\n",
    "The Million Song Dataset has an associated fileset of links for each song in JSON format. It's a nested format that's not very  useful for processing. We need it to be in a CSV format for easy processing.\n",
    "\n",
    "So in order to accomplish that we've built a serverless function that will take data from one S3 bucket that's in JSON format and combine the files and convert them to csv format. The way we initiate that function is through an SQS queue. The benefit of this is that the load processes can co-currently and asyrchronously process the data.\n",
    "\n",
    "This notebook reviews the files in the first S3 bucket and then parses their names/key values and then batches in them in SQS messages 2,000 records at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We are loading the required libraries and using boto3 and the s3 client to create a list of all the files in our input bucket for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "bucket = 'echonest-intermediate-json'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "pages = paginator.paginate(Bucket=bucket)\n",
    "\n",
    "files = [obj['Key'] for page in pages for obj in page['Contents']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the files for processing\n",
    "\n",
    "We only want to process files that are the  JSON extension anything else is outside the scope of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = [f for f in files if f.endswith('.json') and not f.startswith(\"zip\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice the List\n",
    "\n",
    "We have a million files we have to process. We can turn that list into a list of slices - each 2,000 files long. Then later we can just iterate through these slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = 2000\n",
    "\n",
    "slices_files = [json_files[i:i+group_size] for i in range(0, len(json_files), group_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queue the Messages\n",
    "\n",
    "Now we'll iterate through each slice and create a SQS message with the 2,000 associated files and some other information such as the bucket and final file name we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the service resource\n",
    "sqs = boto3.resource('sqs')\n",
    "\n",
    "# Get the queue\n",
    "queue = sqs.get_queue_by_name(QueueName = 'msd-jsontocsv')\n",
    "\n",
    "for i, slice in enumerate(slices_files):\n",
    "    message_body = {'bucket': bucket, 'files': slice, 'fileName': f'data{i}.csv'}\n",
    "\n",
    "    # Create a new message\n",
    "    response = queue.send_message(MessageBody = json.dumps(message_body))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c53e9bdfe80d3c5583b741a8a24bc6913c1a8c4f7d300128dacddd53305a040"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('cse6242-finalproject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
