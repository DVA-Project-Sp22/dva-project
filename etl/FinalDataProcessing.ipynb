{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final ETL Script\n",
    "\n",
    "This takes the intermediate data and turns it into a final CSV\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pip Install PyAthena\n",
    "\n",
    "Sagemaker Notebooks use a default image that doesn't include PyAthena so you need to use pip to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install PyAthena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathena import connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a PyAthena Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(s3_staging_dir='s3://athena-results-c7fhgh8/',\n",
    "               region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Songdata from S3 and clean Track ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songdata = pd.read_sql(\"\"\"\n",
    "\n",
    "select  REVERSE(SUBSTR(REVERSE(SUBSTR(track_id,3)),2)) as track_id\n",
    "    ,   loudness\n",
    "    ,   tempo\n",
    "    ,   artist_familiarity\n",
    "\n",
    "from \\\"millionsongdataset-intermediate\\\".songdata\n",
    "\n",
    "\"\"\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Unique Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tracks = pd.read_sql(\"\"\"\n",
    "\n",
    "SELECT *\n",
    "\n",
    "FROM \\\"millionsongdataset-intermediate\\\".unique_tracks\n",
    "\n",
    "\"\"\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Spotify Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify = pd.read_sql(\"\"\"\n",
    "\n",
    "SELECT *\n",
    "\n",
    "FROM \\\"millionsongdataset-intermediate\\\".spotify\n",
    "WHERE spotify_id <> ''\n",
    "\n",
    "\"\"\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Songdata and Unique Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.merge(songdata, unique_tracks, on=\"track_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the new Merged dataset with Spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, spotify, on=\"song_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the shape of what is left now after all those manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip out the unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['track_id','song_title','artist_name','spotify_id','loudness','tempo','artist_familiarity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use MinMaxScaler to scale the data for visualizations later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data[['loudness','tempo','artist_familiarity']] = scaler.fit_transform(data[['loudness','tempo','artist_familiarity']])\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop any rows that have null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Profanity and Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profanity = pd.read_sql(\"\"\"\n",
    "\n",
    "WITH joined_set AS (\n",
    "SELECT\n",
    "    ly.track_id\n",
    "    , COUNT(ly.word) AS total_unique_words\n",
    "    , COALESCE(COUNT(fw.profanity_word),0) AS total_fw\n",
    "FROM \"millionsongdataset-intermediate\".lyrics AS ly\n",
    "LEFT JOIN \"millionsongdataset-intermediate\".profanity_words AS fw ON ly.word = fw.profanity_word\n",
    "GROUP BY ly.track_id\n",
    ")\n",
    "\n",
    "SELECT\n",
    "      track_id\n",
    "    , CASE WHEN total_unique_words > 0 THEN (total_fw/(total_unique_words*1.0))\n",
    "           ELSE 0.0 \n",
    "      END AS profanity_pct\n",
    "    , CASE WHEN total_fw > 0 THEN 1 ELSE 0\n",
    "    END AS profanity_flag\n",
    "FROM joined_set\n",
    "ORDER BY profanity_pct DESC;\n",
    "\n",
    "\"\"\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Profanity with Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.merge(data, profanity, on=\"track_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill NAs with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['profanity_pct','profanity_flag']] = results[['profanity_pct','profanity_flag']].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to S3 as CSV in Final Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"songdata.csv\", index=False)\n",
    "\n",
    "session = boto3.Session()\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file(\"songdata.csv\", \"millionsongdataset\", \"songdata/songdata.csv\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.8xlarge",
  "interpreter": {
   "hash": "1c53e9bdfe80d3c5583b741a8a24bc6913c1a8c4f7d300128dacddd53305a040"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
